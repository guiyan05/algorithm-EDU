## 基础ReLU（基准）

作为所有变体的基础，明确其特性便于对比优化点。

### 表达式

 $f(x) = \max(0, x)$ 

### 优点

- 计算极快（仅比较和取最大值，无复杂运算）；

- 有效缓解深层网络的梯度消失问题；

- 稀疏激活（负半轴为0），降低网络冗余。

### 缺点

- **死亡ReLU**：负半轴梯度为0，若参数更新导致神经元输入恒负，会永久失活；

- 输出非零中心化，导致后续层输入分布偏移，增加训练难度；

- 函数在 $x=0$ 处不可导，梯度下降时需特殊处理。

### 适用场景

浅层神经网络、基础CNN（如LeNet、早期AlexNet）、对计算速度要求极高的简单任务。


### Leaky ReLU（泄漏ReLU）

### 表达式

 $f(x) = \begin{cases} x, & x > 0 \\ \alpha x, & x \leq 0 \end{cases}$ 

其中 $\alpha$ 为固定小斜率，通常取0.01（也可设0.1/0.001），人工指定无需训练。

### 优点

- 彻底缓解死亡ReLU问题，负半轴保留微小梯度，保证神经元持续更新；

- 计算速度与ReLU几乎一致，无额外开销；

- 稀疏性略低于ReLU，但可通过调整 $\alpha$ 平衡。

### 缺点

-  $\alpha$ 为人工超参数，需针对不同任务调参，无普适值；

- 仍存在输出非零中心化问题，未解决层间输入分布偏移；

- 小 $\alpha$ 对死亡ReLU的缓解效果有限，大 $\alpha$ 会丧失稀疏性。

### 适用场景

CNN（分类/检测/分割）、中层深层网络、需要快速训练且样本量较大的计算机视觉任务。

### PReLU（参数化ReLU）

Leaky ReLU的改进，将固定斜率 $\alpha$ 改为可学习参数，让网络自适应负半轴激活强度。

### 表达式

 $f(x) = \begin{cases} x, & x > 0 \\ \alpha_i x, & x \leq 0 \end{cases}$ 

其中 $\alpha_i$ 为第 $i$ 个通道/神经元的可学习参数，随训练通过梯度下降更新，初始值通常设0.25。

### 优点

- 自适应调整负半轴斜率，对不同任务/数据集的适配性远优于Leaky ReLU；

- 彻底解决死亡ReLU，且能通过学习平衡稀疏性和梯度连续性；

- 仅增加少量参数量（与通道数相关），计算复杂度接近ReLU。

### 缺点

- 额外的可学习参数增加了过拟合风险，小样本任务中效果不佳；

- 训练时需同步更新 $\alpha_i$ ，略增加梯度计算量；

- 仍存在输出非零中心化问题。

### 适用场景

大样本CNN任务（如ImageNet分类、大规模目标检测）、计算机视觉高精度任务、深层卷积网络（如ResNet50/101）。

### RReLU（随机化ReLU）

为PReLU增加随机正则化，训练时随机生成负半轴斜率，测试时固定为均值，兼顾激活自适应和过拟合抑制。

### 表达式

- 训练阶段： $\alpha \sim U(l, u)$ （ $U$ 为均匀分布，通常取 $l=0.1, u=0.3$ ）， $f(x) = \max(\alpha x, x)$ ；

- 测试阶段： $\alpha$ 固定为训练时的期望 $E(\alpha)=(l+u)/2$ ， $f(x) = \max(E(\alpha)x, x)$ 。

### 优点

- 随机斜率起到隐式正则化作用，有效防止过拟合，适配小样本任务；

- 无需手动调参 $\alpha$ ，分布范围 $[l,u]$ 为通用超参数；

- 缓解死亡ReLU的同时，保留了ReLU的稀疏性。

### 缺点

- 训练阶段斜率随机，导致训练过程不稳定，收敛速度略慢；

- 测试阶段用均值近似，存在轻微的预测偏差；

- 仍未解决输出非零中心化问题。

### 适用场景

小样本计算机视觉任务（如细粒度分类、小众数据集检测）、易过拟合的浅层/中层网络、半监督学习任务。


### ELU（指数线性单元）

### 表达式

 $f(x) = \begin{cases} x, & x > 0 \\ \alpha (e^x - 1), & x \leq 0 \end{cases}$ 

其中 $\alpha>0$ 为调参超参数，通常取1.0，负半轴为指数衰减曲线，在 $x\to-\infty$ 时趋近于 $-\alpha$ 。

### 优点

- 输出零中心化：负半轴指数特性让输出均值接近0，解决层间输入分布偏移，加速训练收敛；

- 缓解死亡ReLU，且负半轴梯度随 $x$ 增大而递增，对小值输入更敏感，鲁棒性更强；

- 函数在 $x=0$ 处连续可导，梯度下降更稳定。

### 缺点

- 负半轴包含指数运算，计算复杂度高于ReLU/Leaky ReLU，训练速度稍慢；

-  $\alpha$ 为人工超参数，需针对噪声场景调参；

- 深层网络中仍可能存在梯度衰减。

### 适用场景

含噪声的数据集任务（如低质量图像分类）、GAN生成器/判别器、小样本学习、中层网络。

### SELU（缩放指数线性单元）

ELU的改进，通过固定缩放系数 $\lambda$ 和偏移系数 $\alpha$ ，实现自归一化（Self-Normalization），让深层网络的输入分布自动保持均值0、方差1，无需Batch Normalization（BN）。

### 表达式

 $f(x) = \begin{cases} \lambda x, & x > 0 \\ \lambda \alpha (e^x - 1), & x \leq 0 \end{cases}$ 

其中 $\lambda$ 和 $\alpha$ 为固定常数（由数学推导得出，无需调参）： $\lambda \approx 1.0507$ ， $\alpha \approx 1.67326$ 。

### 优点

- 自归一化：核心优势，深层网络中无需BN/层归一化，减少超参数和计算开销；

- 彻底解决梯度消失/爆炸问题，支持超深层网络（数百层）；

- 输出零中心化、连续可导，鲁棒性优于ELU。

### 缺点

- 计算复杂度高（指数+缩放），训练速度慢；

- 自归一化仅在特定条件下生效：网络需为自归一化网络（SNN），权重初始化和网络结构需严格匹配；

- 不适用于卷积网络，更适配全连接/循环网络。

### 适用场景

超深层网络（如数百层的全连接网络、RNN/LSTM）、无需BN的轻量化网络、序列建模任务。


### GELU（高斯误差线性单元）

融合高斯分布的累积分布函数（CDF），将激活值定义为「输入 $x$ 乘以其属于正区间的概率」，是Transformer（BERT/GPT）的标配激活函数。

### 表达式

精确式： $f(x) = x \cdot \Phi(x)$ ，其中 $\Phi(x)$ 为标准正态分布的CDF；

工程近似式（更常用，避免复杂的积分计算）：

 $f(x) = 0.5x \left(1 + \tanh\left( \sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right) \right) \right)$ 

### 优点

- 全局平滑可导，无分段点，梯度连续且稳定，深层网络中梯度消失风险极低；

- 融合概率思想，激活值与输入的分布相关，泛化性远优于ReLU；

- 适配注意力机制，是NLP大模型的最优选择之一。

### 缺点

- 计算复杂度高（包含双曲正切+三次方），训练速度慢；

- 对硬件加速的支持性略差于ReLU类函数；

- 简单任务/浅层网络中，性能提升不明显，性价比低。

### 适用场景

Transformer大模型（BERT/GPT/LLaMA）、自然语言处理（NLP）全任务、大样本高精度CV任务、生成式AI模型。

### Swish（自门控激活函数）

Google提出，被称为「自门控ReLU」，将sigmoid作为门控因子，实现「输入 $x$ 由自身的sigmoid值门控激活」，平滑性与GELU接近，计算更简单。

### 表达式

 $f(x) = x \cdot \text{sigmoid}(\beta x)$ 

其中 $\beta$ 为超参数： $\beta=1$ 时为Swish-1（最常用）； $\beta$ 也可设为可学习参数，适配不同任务。

### 优点

- 全局平滑可导，梯度连续性优于ReLU/ELU，泛化性强；

- 性能全面优于ReLU，在CNN/Transformer中均有提升；

- 计算复杂度低于GELU，硬件适配性更好。

### 缺点

- 包含sigmoid运算，计算速度慢于ReLU/Leaky ReLU；

- 可学习 $\beta$ 会增加参数量，易过拟合。

### 适用场景

CNN（分类/检测/分割）、Transformer、大模型（CV/NLP）、生成式模型（GAN/VAE）。

### Mish

Swish的改进，将sigmoid替换为tanh(softplus(x))，让函数更平滑、梯度更连续，是目前高精度CV任务的最优激活函数之一。

### 表达式

 $f(x) = x \cdot \tanh\left( \text{softplus}(x) \right) = x \cdot \tanh\left( \ln(1 + e^x) \right)$ 

其中 $\text{softplus}(x)$ 是ReLU的平滑版， $f(x)$ 在全体实数域上无限次可导。

### 优点

- 全局超平滑可导，梯度连续性和泛化性为所有ReLU变体中最优；

- 在高精度CV任务中，效果显著优于Swish/ReLU，提升模型精度；

- 对噪声和小样本的鲁棒性更强。

### 缺点

- 计算复杂度最高（tanh+对数+指数），训练速度最慢；

- 硬件加速支持性差，不适用于轻量化任务；

- 简单任务中性价比极低。

### 适用场景

高精度计算机视觉任务（如目标检测/实例分割/语义分割）、大样本CV大模型、GAN高精度生成任务。


### ReLU6

ReLU的截断版本，将激活值上限限制为6，避免激活值过大导致量化时精度损失。

### 表达式

 $f(x) = \min\left( \max(0, x), 6 \right)$ 

### 优点

- 数值范围固定 $[0,6]$ ，适配网络量化，移动端推理速度快；

- 计算速度与ReLU一致，无额外开销；

- 激活值无爆炸，训练和推理稳定性高。

### 缺点

- 仍存在死亡ReLU和输出非零中心化问题；

- 截断上限6为经验值，部分任务中可调整但无普适性。

### 适用场景

移动端量化CNN（如MobileNetV1/V2）、边缘设备视觉任务、轻量化分类/检测模型。

### Hardswish（硬Swish）

Swish的硬件友好版，用分段线性函数近似Swish的sigmoid门控，在保证性能接近Swish的前提下，大幅降低计算复杂度，适配量化。

### 表达式

 $f(x) = x \cdot \max\left( 0, \min\left( 1, \frac{x + 3}{6} \right) \right)$ 

### 优点

- 仅包含加减乘除和最值运算，计算极快，硬件推理效率远高于Swish/GELU；

- 性能接近Swish，优于ReLU6，量化后精度损失小；

- 无指数/对数运算，适配移动端/边缘设备。

### 缺点

- 在 $x=-3$ 和 $x=3$ 处为分段点，平滑性略差于Swish；

- 简单任务中与ReLU6的性能差异不大。

### 适用场景

移动端量化CNN（如MobileNetV3）、边缘设备计算机视觉任务、轻量化Transformer。
> （注：文档部分内容可能由 AI 生成）
