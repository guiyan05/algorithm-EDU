下面系统性整理 **ReLU 及其常见变体**，包含：
✅ 数学表达式
✅ 优缺点
✅ 适用场景

适合你在 **深度学习理论复习、论文写作、课程作业、面试准备** 中直接使用。

---

## 🔹 1. ReLU（Rectified Linear Unit）

### 表达式

[
f(x)=\max(0,x)
]

### 优点

* 计算简单，收敛快
* 缓解梯度消失问题
* 稀疏激活，提高模型表达能力

### 缺点

* **死亡ReLU问题**：负区间梯度为0，神经元可能永久失活
* 输出非零均值，可能影响训练稳定性

### 适用场景

* CNN / MLP 默认首选激活函数
* 大规模深度网络

---

## 🔹 2. Leaky ReLU

### 表达式

[
f(x)=
\begin{cases}
x, & x>0 \
\alpha x, & x\le0
\end{cases}
\quad (\alpha \approx 0.01)
]

### 优点

* 负区间仍有梯度，缓解死亡ReLU问题
* 实现简单

### 缺点

* 超参数 α 需要人工设定
* 负区间输出仍可能较小，信息利用有限

### 适用场景

* GAN、目标检测、对梯度稳定性要求较高模型

---

## 🔹 3. PReLU（Parametric ReLU）

### 表达式

[
f(x)=
\begin{cases}
x, & x>0 \
a x, & x\le0
\end{cases}
\quad (a \text{可学习})
]

### 优点

* 负区间斜率可自适应学习
* 表达能力强于 Leaky ReLU

### 缺点

* 引入额外参数，略增过拟合风险
* 训练不稳定时可能学到无效斜率

### 适用场景

* 深层 CNN（如人脸识别、视觉模型）
* 对性能要求极高的任务

---

## 🔹 4. RReLU（Randomized ReLU）

### 表达式

训练时：
[
f(x)=
\begin{cases}
x, & x>0 \
r x, & x\le0,\ r\sim U(l,u)
\end{cases}
]

测试时：
[
f(x)=
\begin{cases}
x, & x>0 \
\mathbb{E}[r]x, & x\le0
\end{cases}
]

### 优点

* 具有正则化效果，缓解过拟合
* 避免负区间固定斜率导致的表达受限

### 缺点

* 引入随机性，结果不确定性稍大
* 推理阶段需固定期望值

### 适用场景

* 数据量较小、容易过拟合的 CNN 模型

---

## 🔹 5. ELU（Exponential Linear Unit）

### 表达式

[
f(x)=
\begin{cases}
x, & x>0 \
\alpha(e^x-1), & x\le0
\end{cases}
]

### 优点

* 负区间输出趋近于 −α，使激活均值接近 0
* 收敛速度快于 ReLU

### 缺点

* 计算量略大（指数运算）
* 超参数 α 需要设置

### 适用场景

* 深层网络、需要更稳定训练动态的模型

---

## 🔹 6. SELU（Scaled ELU）

### 表达式

[
f(x)=\lambda
\begin{cases}
x, & x>0 \
\alpha(e^x-1), & x\le0
\end{cases}
]

其中常用：
[
\alpha \approx 1.6733,\quad \lambda \approx 1.0507
]

### 优点

* 具备**自归一化特性**，激活自动趋向均值0方差1
* 可减少对 BatchNorm 的依赖

### 缺点

* 需配合特定初始化方式（LeCun Normal）
* 与 Dropout 不兼容（需用 AlphaDropout）

### 适用场景

* 全连接深层网络、无需 BN 的高效模型

---

## 🔹 7. Softplus

### 表达式

[
f(x)=\ln(1+e^x)
]

### 优点

* ReLU 的光滑版本，处处可导
* 无死亡神经元问题

### 缺点

* 计算开销大
* 梯度较小，训练速度慢于 ReLU

### 适用场景

* 概率建模、变分自编码器（VAE）
* 对光滑性要求高的任务

---

## 🔹 8. GELU（Gaussian Error Linear Unit）

### 表达式（近似）

[
f(x)=x\cdot \Phi(x)\approx 0.5x(1+\tanh[\sqrt{2/\pi}(x+0.0447x^3)])
]

### 优点

* 平滑非线性，概率意义更强
* 在 Transformer 中效果显著优于 ReLU

### 缺点

* 计算复杂度较高
* 理解和实现成本较高

### 适用场景

* Transformer、NLP、大模型

---

## 🔹 9. Swish

### 表达式

[
f(x)=x\cdot \sigma(\beta x)
]

### 优点

* 非单调，性能优于 ReLU（在深网络中）
* 梯度连续、平滑

### 缺点

* 计算成本高
* β 为超参数或可学习参数

### 适用场景

* CNN、EfficientNet、移动端高精度模型

---

## 🔹 10. Mish

### 表达式

[
f(x)=x\tanh(\ln(1+e^x))
]

### 优点

* 更平滑，信息保留能力强
* 在图像分类/检测中常优于 Swish 和 ReLU

### 缺点

* 计算代价高
* 理论解释较复杂

### 适用场景

* 高精度 CNN、目标检测、视觉任务

---

